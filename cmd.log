  987  awk '{print $13}' product_samples.txt > review_body.txt
  988  head review_body.txt 
  989   review_body.txt
  990  head review_body.txt 
  991  head product_samples.txt 
  992  head -n 2 product_samples.txt 
  993  head -1 product_samples.txt 
  994  tail -n+2  product_samples.txt 
  995  tail -n+2  product_samples.txt > review_body.txt 
  996  crontab -e
  997  head cron_tab.txt
  998  pwd
  999  crontab -e
 1000  ls
 1001  head cron_tab.txt
 1002  crontab -e
 1003  script ws6.txt
 1004  ls
 1005  git clone https://github.com/S-phan/Worksheet-6.git
 1006  cd Worksheet-6 
 1007  cd ..
 1008  cp ws6.txt Worksheet-6/
 1009  cp Product.LATEST.0262181533.txt Worksheet-6/
 1010  ls Worksheet-6/
 1011  cd ..
 1012  pwd
 1013  cd~
 1014  pwd
 1015  ls
 1016  cd phans
 1017  ls
 1018  cp products.0262181533.AVGRATING.txt Worksheet-6/
 1019  ls Worksheet-6/
 1020  cp PRODUCTS.0262181533.13-10-2021.txt Worksheet-6/
 1021  cd Worksheet-6/
 1022  git status
 1023  git add . 
 1024  git commit -m "completed wsk 6"
 1025  git push https://github.com/S-phan/Worksheet-6.git
 1026  cd ..
 1027  head productid_top3.txt 
 1028  head product_id.txt
 1029  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1030  head amazon_reviews_us_Books_v1_02.tsv 
 1031  grep 0525947647
 1032  head
 1033  head amazon_reviews_us_Books_v1_02.tsv 
 1034  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1035  head -n 2 amazon_reviews_us_Books_v1_02.tsv
 1036  head -n 3 amazon_reviews_us_Books_v1_02.tsv 
 1037  head -n 2 amazon_reviews_us_Books_v1_02.tsv 
 1038  head -n 2 amazon_reviews_us_Books_v1_02.tsv > product_samples.txt
 1039  script ws7.txt
 1040  git clone https://github.com/S-phan/Worksheet-7.git
 1041  ls
 1042  history > cmds.log
 1043  cp cmds.log Worksheet-6/
 1044  cp cmds.log Worksheet-7/
 1045  cp ws7.txt Worksheet-7/
 1046  cp product_samples.txt Worksheet-7/
 1047  cd Worksheet-6/
 1048  /
 1049  phans@f6linux17:~/Worksheet-6$
 1050  git status
 1051  git add .
 1052  fit commit -m "history file"
 1053  git commit -m "history file"
 1054  git push https://github.com/S-phan/Worksheet-6.git
 1055  cd ..
 1056  cd Worksheet-7/
 1057  git status
 1058  git add .
 1059  git commit -m " completed hw set for worksheet 7"
 1060  git push https://github.com/S-phan/Worksheet-7.git
 1061  head -n 3 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1062  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt
 1063  sed -i 's/<.._\>//g' review_body.txt
 1064  head review_body.txt
 1065  sed -e $'s/,/\\\n/g'
 1066  sed -e $'s/,/\\\n/g' review_body.txt 
 1067  sed 's/and//g' review_body.txt 
 1068  sed 's/or//g' review_body.txt 
 1069  sed 's/if//g' review_body.txt 
 1070  sed 's/in//g' review_body.txt 
 1071  sed 's/it//g' review_body.txt 
 1072  sed -e 's/<[^>]*>//g' review_body.txt 
 1073  sed -e $'s/./\\\n/g' review_body.txt 
 1074  vi review_body.txt 
 1075  sed -e $'s/./\\\n/g' review_body.txt 
 1076  sed “s/,//g” review_body.txt 
 1077  'sed “s/,//g” review_body.txt' 
 1078  sed “s/,//g” review_body.txt 
 1079  sed 's/,//g' review_body.txt 
 1080  sed 's/,//g' review_body.txtv
 1081  sed 's/,//g' review_body.txt
 1082  sed “s/\.//g” review_body.txt 
 1083  sed 's/\.//g' review_body.txt 
 1084  sed -e 's/<[^>]*>//g' review_body.txt 
 1085  sed 's/\.//g' review_body.txt 
 1086  head review_body.txt 
 1087  awk '{print $14}' amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1088  head review_body.txt 
 1089  vi review_body.txt 
 1090  tr --delete '\n' < review_body.txt 
 1091  cat review_body.txt |  tr -d "\n" > review_body.txt 
 1092  head review_body.txt 
 1093  awk '{print $14}' amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1094  head -n 5 amazon_reviews_us_Books_v1_02.tsv 
 1095  >sed -n 3 amazon_reviews_us_Books_v1_02.tsv 
 1096  sed -n 3 amazon_reviews_us_Books_v1_02.tsv 
 1097  sed -n 1 amazon_reviews_us_Books_v1_02.tsv 
 1098  head amazon_reviews_us_Books_v1_02.tsv 
 1099  sed -n 4p amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1100  head review_body.txt 
 1101  sed -n 3p amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1102  head review_body.txt 
 1103  head -n 3 amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1104  head review_body.txt 
 1105  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt 
 1106  head review_body.txt 
 1107  vi review_body.txt 
 1108  sed -i 's/<.._\>//g' review_body.txt 
 1109  head review_body.txt 
 1110  history
 1111  ls
 1112  script ws7.txt
 1113  head -n 3 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1114  vi review_body.txt 
 1115  head -n 3 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1116  vi review_body.txt 
 1117  head -n 4 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1118  vi review_body.txt 
 1119  head amazon_reviews_us_Books_v1_02.tsv 
 1120  head -n 2 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1121  head -n 2 amazon_reviews_us_Books_v1_02.tsv 
 1122  head -n 3 amazon_reviews_us_Books_v1_02.tsv 
 1123  head -n 5 amazon_reviews_us_Books_v1_02.tsv 
 1124  head -n 5 amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1125  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt
 1126  vi review_body.txt 
 1127  sed -e $'s/;/\\\n/g' review_body.txt 
 1128  sed $'s/;/\\\n/g' review_body.txt 
 1129  sed $'s/./\\\n/g' review_body.txt 
 1130  sed 's/\.//g' review_body.txt 
 1131  sed 's/\.//g' review_body.txt >1_review_body.txt 
 1132  sed -i 's/<[a-z][a-z] \/>//g' 1_review_body.txt 
 1133  sed -e 's/<[a-z][a-z] \/>//g' 1_review_body.txt 
 1134  sed 's/\,//g' 1_review_body.txt 
 1135  vi 1_review_body.txt 
 1136  sed -i 's/<.._\>//g' review_body.txt 
 1137  sed -i 's/<.._\>//g' 1_review_body.txt 
 1138  sed -e 's/<[^>]*>//g' 1_review_body.txt 
 1139  sed 's/<[^>]*>//g' 1_review_body.txt 
 1140  sed 's/and//g' 1_review_body.txt 
 1141  vi 1_review_body.txt 
 1142  sed -e 's/and//g' 1_review_body.txt 
 1143  vi 1_review_body.txt 
 1144  sed 's/\<and\>//g' 1_review_body.txt 
 1145  vi 1_review_body.txt 
 1146  sed -i 's/\<and\>//g' 1_review_body.txt 
 1147  head -n 5 amazon_reviews_us_Books_v1_02.tsv > review_body.txt 
 1148  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt
 1149  sed -i 's/<.._\>//g' review_body.txt
 1150  head review_body.txt 
 1151  sed -i $'s/,/\\\n/g' review_body.txt 
 1152  sed -i $'s/./\\\n/g'review_body.tx
 1153  sed -i $'s/./\\\n/g'review_body.txt
 1154  sed -i $'s/;/\\\n/g'review_body.txt
 1155  sed -i 's/;//g' review_body.txt
 1156  head review_body.txt 
 1157  sed -i 's/.//g' review_body.txt
 1158  sed -i 's/,//g' review_body.txt
 1159  head review_body.txt 
 1160  vi review_body.txt 
 1161  head -n 5 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1162  head review_body.txt 
 1163  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt
 1164  head review_body.txt 
 1165  sed -i 's/<.._\>//g' review_body.txt
 1166  head review_body.txt 
 1167  sed -i 's/;//g' review_body.txt 
 1168  head review_body.txt 
 1169  sed -i 's/,//g' review_body.txt
 1170  sed -i 's/.//g' review_body.txt
 1171  head review_body.txt 
 1172  head -n 5 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1173  head review_body.txt
 1174  ```bash
 1175  head -n 3 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1176   2007  head review_body.txt
 1177   2008  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt
 1178   2009  head review_body.txt
 1179   2010  vi review_body.txt
 1180   2011  sed -i 's/<.._\>//g' review_body.txt
 1181   2012  head review_body.txt
 1182  sed -i $'s/,/\\\n/g' #remove comma 
 1183  sed -i $'s/./\\\n/g' review_body.txt #remove dot 
 1184  sed -i $'s/;/\\\n/g'review_body.txt #remove semi dot
 1185  sed -i 's/and//g' review_body.txt
 1186  sed 's/or//g' review_body.txt
 1187  sed 's/if//g' review_body.txt
 1188  sed 's/in//g' review_body.txt
 1189  sed 's/it//g'
 1190  sed -i 's/,//g' review_body.txt
 1191  sed -i 's/;//g' review_body.txt
 1192  sed -i 's/\.//g' review_body.txt
 1193  head -n 5 amazon_reviews_us_Books_v1_02.tsv > review_body.txt
 1194  head review_body.txt 
 1195  sed -i 's/<[a-z][a-z] \/>//g' review_body.txt
 1196  head review_body.txt 
 1197  sed -i 's/<.._\>//g' review_body.txt
 1198  head review_body.txt 
 1199  sed -i 's/\.//g' review_body.txt
 1200  head review_body.txt 
 1201  sed -i 's/;//g' review_body.txt
 1202  sed -i 's/,//g' review_body.txt
 1203  sed -i 's/and//g' review_body.txt
 1204  sed -i "s/\<$i\>//g" 1_review_body.txt 
 1205  head 1_review_body.txt 
 1206  vi 1_review_body.txt 
 1207  head -n 1 1_review_body.txt 
 1208  awk '{print $14}' 1_review_body.txt > only_review_body.txt 
 1209  head only review_body.txt 
 1210  script ws7.txt
 1211  ls
 1212  cd Worksheet-7/
 1213  ls
 1214  rm ws7.txt 
 1215  rm product_samples.txt
 1216  rm cmds.log
 1217  cd ..
 1218  cp review_body.txt Worksheet-7/
 1219  cp 1_review_body.txt Worksheet-7/
 1220  history > cmds.log 
 1221  cp cmds.log Worksheet-7/
 1222  cp only_review_body.txt Worksheet-7/
 1223  cd Worksheet-7/
 1224  git status
 1225  git add . 
 1226  cd ..
 1227  cp ws7.txt Worksheet-7/
 1228  cd Worksheet-7
 1229  ls
 1230  git status
 1231  git add .
 1232  git commit -m " revised hw"
 1233  git push https://github.com/S-phan/Worksheet-7.git
 1234  ls
 1235  cd Assignment-2
 1236  ls
 1237  cd ..
 1238  head customerId_helpfulness_100.txt
 1239  head  customerID_helpfulness_100.txt
 1240  head customer_and_helpful.txt
 1241  head  product_id_helpfulness.txt
 1242  head product_id.txt.txt 
 1243  vi product_id.txt.txt
 1244  cd~
 1245  pwd
 1246  139
 1247  awk '{ sum += $2; n++ } END { if (n > 0) print sum / n; }' product_id.txt.txt
 1248  head product_id.txt.txt 
 1249  awk '{ sum += $2; n++ } END { if (n > 0) print sum / n; }' product_id.txt.txt
 1250  awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' product_id.txt.txt
 1251  touch mediantest.txt
 1252  vi mediantest.txt
 1253  awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' mediantest.txt 
 1254  xxd -r -p mediantest.txt > binary_dump
 1255  head binary_dump 
 1256  pwd
 1257  xxd binary_dump 
 1258  xxd -c \ mediantest.txt > binary_dump
 1259  xxd -c  mediantest.txt > binary_dump
 1260  bc mediantest.txt 
 1261  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") "$1"
 1262  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") "$1" mediantest.txt 
 1263  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") "$1" 
 1264  for i in `mediantest.txt`, do printf "%s %08d 0x%02x\n" "$i" $(bc <<< "ibase=10;obase=2;$i") "$i"; done
 1265  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$i" $(bc <<< "ibase=10;obase=2;$i") "$i"; done
 1266  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$i") "$1"; done
 1267  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") "$1" 
 1268  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") "$1" mediantest.txt 
 1269  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") mediantest.txt 
 1270  printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$1") "mediantest.txt"
 1271  for i 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$i") "$1"; done
 1272  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$i") "$1"; done
 1273  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" $(bc >>> "ibase=10;obase=2;$i") "$1"; done
 1274  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" $(bc < "ibase=10;obase=2;$i") "$1"; done
 1275  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" $(bc <<< "ibase=10;obase=2;$i") "$1"; done
 1276  head mediantest.txt 
 1277  for i in 'mediantest.txt'; do printf "%s %08d 0x%02x\n" "$1" ; done
 1278  for i in `mediantest.txt`; do printf "%s %08d 0x%02x\n" "$i" $(bc <<< "ibase=10;obase=2;$i") "$i"; done
 1279  for i in `mediantest.txt`; do printf("%s %s %x\n", $1, bits2str($1), $1); done
 1280  awk -f awkscr.awk mediantest.txt 
 1281  echo mediantest.txt 
 1282  cat mediantest.txt| bc 
 1283  echo "obase=2;mediantest.txt"| bc 
 1284  echo "obase=2 ; mediantest.txt"| bc 
 1285  awk '{print "ibase=10;obase=2;" $1}' mediantest.txt | bc | xargs printf "%08d\n"
 1286  ls
 1287  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1288  awk '{print $2,$9,$10} > ID_help_votes.txt
 1289  awk '{print $2,$9,$10}' > ID_help_votes.txt
 1290  awk '{print $2,$9,$10}' amazon_reviews_us_Books_v1_02.tsv> ID_help_votes.txt
 1291  head ID_help_votes.txt 
 1292  awk -F '{print $2,$9,$10}' amazon_reviews_us_Books_v1_02.tsv> ID_help_votes.txt
 1293  awk '{print $2,$9}' amazon_reviews_us_Books_v1_02.tsv> ID_help_votes.txt
 1294  head ID_help_votes.txt 
 1295  awk '{print $2,$10}' amazon_reviews_us_Books_v1_02.tsv> ID_help_votes.txt
 1296  head ID_
 1297  head ID_help_votes.txt 
 1298  awk '{print $9}' amazon_reviews_us_Books_v1_02.tsv> ID_help_votes.txt
 1299  head ID_
 1300  head ID_help_votes.txt 
 1301  head amazon_reviews_us_Books_v1_02.tsv 
 1302  cut -d " " -f 9 amazon_reviews_us_Books_v1_02.tsv | head
 1303  cut -d " " -f 9 amazon_reviews_us_Books_v1_02.tsv 
 1304  cut -d "" -f 9 amazon_reviews_us_Books_v1_02.tsv 
 1305  phans@f6linux17:~$ ^C
 1306  cut -f 9 amazon_reviews_us_Books_v1_02.tsv
 1307  awk '{print $9}'
 1308  awk '{print $9}' amazon_reviews_us_Books_v1_02.tsv 
 1309  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1310  cut -d "     " -f 2,9,10 amazon_reviews_us_Books_v1_02.tsv > ID_help_vote.txt
 1311  cut -d "     " 2,9,10 amazon_reviews_us_Books_v1_02.tsv > ID_help_vote.txt
 1312  cut -d "     " -f 2,9,10 amazon_reviews_us_Books_v1_02.tsv > ID_help_vote.txt
 1313  cut "     " -f 2,9,10 amazon_reviews_us_Books_v1_02.tsv > ID_help_vote.txt
 1314  cut -d "	"-f 2,9,10 amazon_reviews_us_Books_v1_02.tsv > ID_help_vote.txt
 1315  cut -d "	" -f 2,9,10 amazon_reviews_us_Books_v1_02.tsv > ID_help_vote.txt
 1316  head ID_help_vote.txt
 1317  cut -d "	" -f 4,9,10 amazon_reviews_us_Books_v1_02.tsv > product_help_vote.txt
 1318  head product_help_vote.txt 
 1319  ls
 1320  mkdir Ass2
 1321  ls
 1322  cd Ass2
 1323  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1324  cd..
 1325  cd ..
 1326  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1327  awk -F "\t" '{print $2}' amazon_reviews_us_Books_v1_02.tsv | sort | uniq | wc
 1328  awk -F "\t" '{print $2}' amazon_reviews_us_Books_v1_02.tsv  | sort | uniq -c | sort -n -r | head -n 100  > top100customers
 1329  for i in `cat top100customers | awk '{print $2}'` ; do echo "$i"; grep $i amazon_reviews_us_Books_v1_02.tsv | awk -F "\t" '{print $8,$9}' > ~/customers/$i.txt ; done
 1330  ls
 1331  cd customers
 1332  ls
 1333  cd ..
 1334  mkdir products
 1335  cd products
 1336  ls
 1337  cd ..
 1338  mkdir product
 1339  awk -F "\t" '{print $4}' amazon_reviews_us_Books_v1_02.tsv  | sort | uniq -c | sort -n -r | head -n 100  > top100products
 1340  for i in `cat top100products | awk '{print $2}'` ; do echo "$i"; grep $i amazon_reviews_us_Books_v1_02.tsv | awk -F "\t" '{print $8,$9}' > ~/product/$i.txt   ; done
 1341  cd product
 1342  ls
 1343  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0}
 1344  vi 0060582510.txt 
 1345  cd product
 1346  sort -n -k 1 0060582510.txt | awk '{ a[i++]=$1 } END { print a[int(i/2)]; }'
 1347  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0} ' 0060582510.txt  > 0060582510.Binary.txt 
 1348  vi 0060582510.Binary.txt 
 1349  ../datamash-1.3/datamash -W ppearson 1:2 < 0060582510.Binary.txt 
 1350  gnuplot
 1351  ls 
 1352  sort 0060582510.Binary.txt > 0060582510.Binary.txt.sorted.txt
 1353  awk '{print NR, $1}' 0060582510.Binary.txt.sorted.txt > 0060582510.Binary.txt.sorted.txt.rating
 1354  awk '{print NR, $1}' 0060582510.Binary.txt.sorted.txt > 00682510.Binary.txt.sorted.txt.rating
 1355  cd product
 1356  awk '{print NR, $1}' 0060582510.Binary.txt.sorted.txt > 0060582510.Binary.txt.sorted.txt.rating
 1357  awk '{print NR, $2}' 0060582510.Binary.txt.sorted.txt > 0060582510.Binary.txt.sorted.txt.helpful
 1358  cd product
 1359  ls
 1360  plot '0060582510.Binary.txt.sorted.txt.helpful' with linespoints linestyle 1 linecolor 7 title "helpful", '0060582510.Binary.txt.sorted.txt.rating' with linepoints linestyle 1 linecolor 5 title "rating"
 1361  install plotutils
 1362  sudo apt-get update -y
 1363  sudo apt-get install -y plotutils
 1364  gnuplot
 1365  apt install gnuplot-nox
 1366  apt install gnuplot-qt
 1367  cd ..
 1368  gnuplot
 1369  sudo apt-get install libncurses5-dev
 1370  sudo apt-get install ncurses-dev
 1371  cd product
 1372  sort -n -k 1 0060582510.txt | awk '{ a[i++]=$1 } END { print a[int(i/2)]; }'
 1373  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0} ' 0060582510.txt  > 0060582510.Binary.txt
 1374  ../datamash-1.3/datamash -W ppearson 1:2 < 0060582510.Binary.txt
 1375  ls
 1376  vi 0060582510.Binary.txt 
 1377  cd ..
 1378  ls
 1379  cd customers
 1380  ls
 1381  sort -n -k 1 20595117.txt | awk '{ a[i++]=$1 } END { print a[int(i/2)]; }'
 1382  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0} ' 0060582510.txt  > 0060582510.Binary.txt
 1383  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0} ' 20595117.txt  > 20595117.Binary.txt
 1384  vi 20595117.Binary.txt 
 1385  sort 20595117.Binary.txt > 20595117.Binary.txt.sorted.txt
 1386  awk '{print NR, $1}' 20595117.Binary.txt.sorted.txt > 20595117.Binary.txt.sorted.txt.rating
 1387  awk '{print NR, $2}' 20595117.Binary.txt.sorted.txt > 20595117.Binary.txt.sorted.txt.helpful
 1388  cd ..
 1389  cd product
 1390  awk '{print NR, $1}' 0060582510.Binary.txt.sorted.txt > 0060582510.Binary.txt.sorted.txt.rating
 1391  awk '{print NR, $2}' 0060582510.Binary.txt.sorted.txt > 0060582510.Binary.txt.sorted.txt.helpful
 1392  cd ..
 1393  gnuplot
 1394  apt install gnuplot-nox
 1395  apt install gnuplot-qt
 1396  echo unable to download gnuplot
 1397  echo question 7 yes there is more meaning since you can better compare the data point to eachother
 1398  awk -F "\t" '{print $14}' amazon_reviews_us_Books_v1_02.tsv | head -n 20 > review_body1.txt
 1399  sed -e 's/<[^>]*>//g' review_body1.txt
 1400  sed -i 's/<[^>]*>//g' review_body1.txt 
 1401  sed 's/or//g' review_body1.txt
 1402  sed -i 's/or//g' review_body1.txt
 1403  sed -i 's/and//g' review_body1.txt
 1404  sed -i 's/it//g' review_body1.txt
 1405  sed -i 's/in//g' review_body1.txt
 1406  sed -i 's/if//g' review_body1.txt
 1407  tr " " "\n" < review_body1.txt | sort | uniq -c
 1408  tr " " "\n" < review_body1.txt | sort | uniq -c | sort -n
 1409  sed -i 's/the//g' review_body1.txt
 1410  sed -i 's/of//g' review_body1.txt
 1411  sed -i 's/to//g' review_body1.txt
 1412  sed -i 's/that//g' review_body1.txt
 1413  sed -i 's/is//g' review_body1.txt
 1414  sed -i 's/this//g' review_body1.txt
 1415  tr " " "\n" < review_body1.txt | sort | uniq -c | sort -n
 1416  sed -i 's/a//g' review_body1.txt
 1417  sed -i 's/th//g' review_body1.txt
 1418  sed -i 's/f//g' review_body1.txt
 1419  tr " " "\n" < review_body1.txt | sort | uniq -c | sort -n
 1420  gnuplot
 1421  apt install gnuplot-nox
 1422  su root
 1423  su - root
 1424  su -
 1425  sudo -i
 1426  su
 1427  sudo passwd root
 1428  ile.  This incident will be reported.
 1429  phans@f6linux17:~$
 1430  usermod -a -G sudo phans
 1431  sudo nano /etc/sudoers
 1432  awk -F "\t" '{print $14}' amazon_reviews_us_Books_v1_02.tsv | head -n 10 > review_body1.txt
 1433  head -n 10 review_body.txt 
 1434  awk -F "\t" '{print $14}' amazon_reviews_us_Books_v1_02.tsv | head -n 10 > review_body1.txt
 1435  tr " " "\n" < review_body1.txt | sort | uniq -c
 1436  tr " " "\n" < review_body1.txt | sort | uniq -c | sort -n | less
 1437  script a3.txt
 1438  history > cmds.log 
 1439  vi a3.txt
 1440  git clone https://github.com/S-phan/Assignments-3-.git
 1441  cp a3.txt cmds.log Assignments-3-/
 1442  cd  Assignments-3-/
 1443  ls
 1444  cd ..
 1445  ls
 1446  cd Assignments-3-
 1447  git status
 1448  git add .
 1449  git commit -m " assignment 3"
 1450  git push https://github.com/S-phan/Assignments-3-.git
 1451  wget http://ftp.cstug.cz/pub/CTAN/graphics/gnuplot/5.2.6/gnuplot-5.2.6.tar.gz
 1452  gunzip gnuplot-5.2.6.tar.gz
 1453  tar xvf gnuplot-5.2.6.tar
 1454  ./configure
 1455  make
 1456  cd ~
 1457  history
 1458  cd product
 1459  ls
 1460  cd..
 1461  cd ..
 1462  cd gnuplot-5.2.6/
 1463  plot '0060582510.Binary.txt.sorted.txt.helpful' with linespoints linestyle 1 linecolor 7 title "helpful", '0060582510.Binary.txt.sorted.txt.rating' with linepoints linestyle 1 linecolor 5 title "rating"
 1464  apt install plotutils
 1465  plot exp(-x**2 / 2)
 1466  plot [-4:4] exp(-x**2 / 2), x**2 / 16
 1467  plot sin(x)/x
 1468  plot '0060392452.txt.BINARY.txt.sorted.helpful' with linespoints linestyle 1 linecolor 7 title "helpful", '0060392452.txt.BINARY.txt.sorted.ratings' with linespoints linestyle 1 linecolor 6 title "rating"
 1469  whereis gnuplot
 1470  ls
 1471  cd product
 1472  whereis gnuplot
 1473  wget http://ftp.cstug.cz/pub/CTAN/graphics/gnuplot/5.2.6/gnuplot-5.2.6.tar.gz
 1474  cd gnuplot-5.2.6/
 1475  ./configure
 1476  make check
 1477  ./src/gnuplot
 1478  cd ..
 1479  ls
 1480  cd gnuplot-5.2.6/
 1481  ls
 1482  gnuplot
 1483  cd ..
 1484  gnuplot
 1485  cd ..
 1486  gnuplot
 1487  cd gnuplot
 1488  ls
 1489  gnuplot
 1490  cd product
 1491  cd gnuplot-5.2.6/
 1492  vi install-sh 
 1493  plot
 1494  ./configure
 1495  ./src/gnuplot
 1496  cd ..
 1497  ./src/gnuplot
 1498  ls
 1499  cd gnuplot-5.2.6/
 1500  ./src/gnuplot
 1501  cd product
 1502  cd ..
 1503  cd product
 1504  ls
 1505  cp 0060582510.Binary.txt.sorted.txt.helpful gnuplot-5.2.6
 1506  cp 0060582510.Binary.txt.sorted.txt.rating gnuplot-5.2.6
 1507  cd gnuplot-5.2.6/
 1508  cd ..
 1509  cd gnuplot-5.2.6/
 1510  ./src/gnuplot
 1511  cd ..
 1512  cd product 
 1513  ls
 1514  cd 0060582510.Binary.txt.sorted.txt.helpful ..
 1515  cd.. 0060582510.Binary.txt.sorted.txt.helpful
 1516  cp  0060582510.Binary.txt.sorted.txt.helpful/ ..
 1517  cp  0060582510.Binary.txt.sorted.txt.helpful ../
 1518  cd ..
 1519  ls
 1520  cd product
 1521  cp  0060582510.Binary.txt.sorted.txt.rating ../
 1522  cd ..
 1523  cd gnuplot-5.2.6/
 1524  ./src/gnuplot
 1525  cd ..
 1526  cd product
 1527  gnuplot-5.2.6/
 1528  ./src/gnuplot
 1529  cd gnuplot-5.2.6/
 1530  ./src/gnuplot
 1531  cd ..
 1532  ls
 1533  vi 0060582510.Binary.txt.sorted.txt.helpful
 1534  vi 0060582510.Binary.txt.sorted.txt.rating 
 1535  cd product
 1536  ls
 1537  head 0060582510.Binary.txt.sorted.txt   
 1538  vi 0060582510.Binary.txt.sorted.txt   
 1539  head 0060582510.txt
 1540  sort -n -k 1 0060582510.txt | awk '{ a[i++]=$1 } END { print a[int(i/2)]; }'
 1541  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0} ' 0060582510.txt
 1542  vi 0060582510.Binary.txt
 1543  ls
 1544  vi 0060582510.Binary.txt.sorted.txt
 1545  vi 0060582510.Binary.txt.sorted.txt| uniq > 0060582510.Binary.txt.sorted.uniq.txt
 1546  ls
 1547  cd product
 1548  ls
 1549  vi  0060582510.Binary.txt.sorted.uniq.txt
 1550  cat 0060582510.Binary.txt.sorted.txt| uniq > 0060582510.Binary.txt.sorted.uniq.txt 
 1551  vi 0060582510.Binary.txt.sorted.uniq.txt
 1552  ls
 1553  vi 0060582510.Binary.txt.sorted.txt
 1554  ls
 1555  vi 0060582510.Binary.txt.sorted.txt.rating
 1556  vi 0060582510.Binary.txt.sorted.txt.helpful
 1557  0060582510.Binary.txt
 1558  vi 0060582510.Binary.txt
 1559  cd ..
 1560  ls
 1561  head top100products 
 1562  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1563  cd product
 1564  ls
 1565  awk '{print NR, $1}' 0060582510.Binary.txt.sorted.txt > 0060582510.Binary.txt.sorted.txt.rating
 1566  head 0060582510.Binary.txt.sorted.txt.rating
 1567  vi 0060582510.Binary.txt.sorted.txt.rating
 1568  vi  00682510.Binary.txt.sorted.txt.rating
 1569  vi 0060582510.Binary.txt
 1570  head 0060582510.txt
 1571  cd product
 1572  sort -n -k 1 0060582510.txt | awk '{ a[i++]=$1 } END { print a[int(i/2)]; }'
 1573  head 0060582510.Binary.txt
 1574  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,0} ' 0060582510.txt  > 0060582510.Binary.txt
 1575  awk '{if (int($median) < int($1)) print $1,1 ; else print $1,0} ' 0060582510.txt  > 0060582510.rating.Binary.txt
 1576  head 0060582510.rating.Binary.txt
 1577  tail 0060582510.rating.Binary.txt
 1578  awk '{if (int($median) < int($2)) print $1,1 ; else print $1,1} ' 0060582510.txt  > 0060582510.rating.Binary.txt
 1579  head 0060582510.rating.Binary.txt
 1580  vi 0060582510.rating.Binary.txt
 1581  awk '{if (int($median) < int($1)) print $1,1 ; else print $1,0} ' 0060582510.txt  > 0060582510.rating.Binary.txt
 1582  vi 0060582510.rating.Binary.txt
 1583  vi 0060582510.txt 
 1584  product
 1585  cd product
 1586  ls
 1587  vi 0060582510.Binary.txt.sorted.txt.rating 
 1588  ls
 1589  cd ..
 1590  cd customers/
 1591  ls
 1592  vi 20595117.Binary.txt.sorted.txt.helpful
 1593  vi 20595117.Binary.txt.sorted.txt.rating
 1594  head amazon_reviews_us_Books_v1_02.tsv 
 1595  head amazon_reviews_us_Books_v1_02.tsv | awk 'verified'
 1596  head amazon_reviews_us_Books_v1_02.tsv | awk '/verified/'
 1597  amazon_reviews_us_Books_v1_02.tsv | awk '/verified/'
 1598  cat amazon_reviews_us_Books_v1_02.tsv | awk '/verified/'
 1599  cat amazon_reviews_us_Books_v1_02.tsv | awk '/verified/' > verified.txt
 1600  head verified.txt 
 1601  awk '/verified/' amazon_reviews_us_Books_v1_02.tsv 
 1602  touch hello
 1603  vi hello
 1604  awk '/^hello1$/' hello
 1605  awk '/verified$/' amazon_reviews_us_Books_v1_02.tsv 
 1606  awk '/verified/' amazon_reviews_us_Books_v1_02.tsv 
 1607  awk '/verified/' amazon_reviews_us_Books_v1_02.tsv | uniq -c 
 1608  cat amazon_reviews_us_Books_v1_02.tsv | grep verified 
 1609  awk '/verified$/' amazon_reviews_us_Books_v1_02.tsv 
 1610  awk '/verified/' amazon_reviews_us_Books_v1_02.tsv > verified.txt 
 1611  tr " " "\n" < verified.txt | sort | uniq -c
 1612  tr " " "\n" < verified.txt | sort | uniq -c | head
 1613  tr " " "\n" < verified.txt | sort | uniq -c | tail
 1614  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1615  tr " " "\n" < verified.txt | sort -k14 | uniq -c | tail
 1616  tr " " "\n" < verified.txt | sort -k14 | uniq -c | head
 1617  tr " " "\n" < verified.txt | uniq -c | sort -k14 | head
 1618  tr " " "\n" < verified.txt | sort -k14 | uniq -c | head
 1619  tr " "\n" < verified.txt | sort -k14 | uniq -c | head
 1620  tr " "\n" < verified.txt | sort -k14 | uniq -c | head
 1621  tr " " " < verified.txt | sort -k14 | uniq -c | head
 1622  tr " " "\n" < verified.txt | sort -k14 | uniq -c | head
 1623  tr " " "\n" < verified.txt | sort -k14 | uniq -c > test.txt
 1624  vi test.txt
 1625  tr " " "\n" < verified.txt | sort -r -k14 | uniq -c > test.txt
 1626  vi test.txt
 1627  tr " " "\n" < verified.txt | sort -r -k14 | uniq -c -r > test.txt
 1628  tr " " "\n" < verified.txt | sort -n -k14 | uniq -c > test.txt
 1629  vi test
 1630  tr " " "\n" < verified.txt | sort -r -k14 | uniq -c -r > test.txt
 1631  tr " " "\n" < verified.txt | sort -r -k14 | uniq -c > test.txt
 1632  tr " " "\n" < verified.txt | sort -r -k14 | uniq -c | sort -n > test.txt
 1633  vi test.txt 
 1634  tail test.txt 
 1635  awk '/verified/' amazon_reviews_us_Books_v1_02.tsv > verified.txt
 1636  awk '/unverified/' amazon_reviews_us_Books_v1_02.tsv > unverified.txt
 1637  vi verified.txt 
 1638  tr " " "\n" < verified.txt | sort -r -k14 | uniq -c | sort -n > frequent.verified.txt
 1639  head frequent.verified.txt 
 1640  tail frequent.verified.txt 
 1641  tr " " "\n" < unverified.txt | sort -r -k14 | uniq -c | sort -n > frequent.unverified.txt
 1642  tail frequent.unverified.txt 
 1643  script ws8.txt
 1644  history > cmds.log 
 1645  git clone https://github.com/S-phan/Worksheet-8.git
 1646  cp ws8.txt Worksheet-8/
 1647  cp frequent.unverified.txt Worksheet-8/
 1648  cp frequent.verified.txt Worksheet-8/
 1649  cp cmds.log Worksheet-8/
 1650  cd Worksheet-8/
 1651  git status
 1652  git add .
 1653  git add.
 1654  git add .
 1655  git commit -m "worksheet 8"
 1656  git push https://github.com/S-phan/Worksheet-8.git
 1657  awk `/ring/ {print}` amazon_reviews_us_Books_v1_02.tsv 
 1658  wk ‘/ring/ { print }’ amazon_data
 1659  awk ‘/ring/ { print }’ amazon_reviews_us_Books_v1_02.tsv 
 1660  awk { print } amazon_reviews_us_Books_v1_02.tsv 
 1661  awk '/[Ll]ord|king/ {print}' amazon_reviews_us_Books_v1_02.tsv 
 1662  awk '/[Ll]ord|king/ {print}' amazon_reviews_us_Books_v1_02.tsv  | head 
 1663  awk '/[Ll]ord|king/ {print}' amazon_reviews_us_Books_v1_02.tsv  | head > test.txt
 1664  vi test.txt
 1665  awk 'BEGIN {print "howdy, folks"} //' amazon_reviews_us_Books_v1_02.tsv 
 1666  awk 'BEGIN {print "howdy, folks"} //' amazon_reviews_us_Books_v1_02.tsv  > test.txt
 1667  vi test.txt 
 1668  head test.txt 
 1669  vi test.txt 
 1670  head amazon_reviews_us_Books_v1_02.tsv 
 1671  head amazon_reviews_us_Books_v1_02.tsv > test.txt 
 1672  vi test.txt 
 1673  rm test.txt
 1674  touch test.txt
 1675  vi test.txt 
 1676  awk '/Phoenix/,/time/ {print}' test.txt 
 1677  vi test.txt 
 1678  awk '/Phoenix/,/time/ {print}' test.txt 
 1679  vi test.txt 
 1680  awk '/Phoenix/,/time/ {print}' test.txt 
 1681  . script
 1682  sh script
 1683  ./test1
 1684  touch test1
 1685  vi test1
 1686  chmod 755 test1
 1687  ./test1
 1688  vi test1 
 1689  ./test1
 1690  test
 1691  vi test1 
 1692  ./test1
 1693  sed '3q' amazon_file
 1694  sed '3q' amazon_reviews_us_Books_v1_02.tsv 
 1695  sed 's/ *|/|/g' amazon_reviews_us_Books_v1_02.tsv 
 1696  vi test1
 1697  wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
 1698  unzip trainingandtestdata.zip 
 1699  ls
 1700  unzip trainingandtestdata.zip 
 1701  ls
 1702  wc training.1600000.processed.noemoticon.csv 
 1703  ls -la 
 1704  training.1600000.processed.noemoticon.csv
 1705  head training.1600000.processed.noemoticon.csv
 1706  head -n 1 training.1600000.processed.noemoticon.csv
 1707  touch A4_script.sh
 1708  vi A4_script.sh 
 1709  chmod 755 A4_script.sh 
 1710  ./A4_script.sh 
 1711  touch file1.txt
 1712  vi file1.txt
 1713  touch file2.txt
 1714  vi file2.txt 
 1715  ./A4_script.sh 
 1716  vi A4_script.sh 
 1717  ./A4_script.sh 
 1718  cut -d "     " -f 2,9 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1719  cut -d "	" -f 2,9 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1720  head customer_id_helpfulness.txt
 1721  head -n 1 amazon_reviews_us_Books_v1_02.tsv
 1722  cut -d "	" -f 2,13 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1723  head customer_id_helpfulness.txt
 1724  cut -d "	" -f 2,9,13 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1725  head customer_id_helpfulness.txt
 1726  sort -nk2 --reverse  customer_id_helpfulness.txt | head
 1727  vi randomsample.sh
 1728  cut -d "     " -f 2,9,13 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1729  cut -d "	" -f 2,9,13 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1730  sort -nk2 --reverse  customer_id_helpfulness.txt | head -n 100 > sorted.customer_and_helpful.txt
 1731  cut -d "	" -f 3 sorted.customer_and_helpful.txt > REVIEWS/reviewID.txt
 1732  head sorted.customer_and_helpful.txt
 1733  sed -i 's/or//g' sorted.customer_and_helpful.txt 
 1734  sed -i 's/and//g' sorted.customer_and_helpful.txt 
 1735  sed -i 's/my//g' sorted.customer_and_helpful.txt 
 1736  sed -i 's/you//g' sorted.customer_and_helpful.txt 
 1737  sed -i 's/she//g' sorted.customer_and_helpful.txt 
 1738  sed -i 's/he//g' sorted.customer_and_helpful.txt 
 1739  sed -i 's/a//g' sorted.customer_and_helpful.txt 
 1740  sed -i 's/and//g' sorted.customer_and_helpful.txt 
 1741  sed -i 's/but//g' sorted.customer_and_helpful.txt 
 1742  sed -i 's/an//g' sorted.customer_and_helpful.txt 
 1743  sed -i 's/you//g' sorted.customer_and_helpful.txt 
 1744  sed -i 's/you'd//g' sorted.customer_and_helpful.txt 
 1745  q
 1746  sed -i 's/it//g' sorted.customer_and_helpful.txt 
 1747  q
 1748  sed -i 's/it//g' sorted.customer_and_helpful.txt 
 1749  vi A4_script
 1750  vi A4_script.sh 
 1751  ./A4_script.sh 
 1752  ls
 1753  ./A4_script.sh 
 1754  vi training.1600000.processed.noemoticon.csv
 1755  cut -d "	" -f 6 training.1600000.processed.noemoticon.csv > tweet_text.txt
 1756  ./A4_script.sh 
 1757  cut -d "	" -f 3 sorted.customer_and_helpful.txt > review_body
 1758  ./A4_script.sh 
 1759  vi tweet_text.txt
 1760  awk '{print $7}' tweet_text.txt| head
 1761  awk '{print $7}' tweet_text.txt > tweet_text2.txt
 1762  ./A4_script.sh 
 1763  head tweet_text2.txt
 1764  head sorted.customer_and_helpful.txt
 1765  ./A4_script.sh 
 1766  head review_body
 1767  ./A4_script.sh 
 1768  vi A4_script.sh 
 1769  ./A4_script.sh 
 1770  comm -12 <(sort tweet_text2.txt) <(sort review_body)
 1771  time comm -12 <(sort tweet_text2.txt) <(sort review_body)
 1772  cut -d "	" -f 2,9,13 amazon_reviews_us_Books_v1_02.tsv > customer_id_helpfulness.txt
 1773  sort -nk2 --reverse  customer_id_helpfulness.txt | head -n 100 > sorted.customer_and_helpful.txt
 1774  head sorted.customer_and_helpful.txt
 1775  awk '{ print $3}' sorted.customer_and_helpful.txt > review_body 
 1776  sed -i -e 's/ing//g' review_body
 1777  sed -i 's/my//g' review_body
 1778  sed -i 's/you//g' review_body
 1779  sed -i 's/we//g' review_body
 1780  cut -d "	" -f 6 training.1600000.processed.noemoticon.csv > tweet_text2.txt
 1781  vi A4_script.sh 
 1782  echo i wrote the code before recording
 1783  comm -12 tweet_text.txt review_body3.txt
 1784  time comm -12 <(sort file1) <(sort file2)
 1785  echo I must have set up the file incorrectly but my top words are A I and not
 1786  script a4.txt
 1787  vi A4_script.sh 
 1788  ./A4_script.sh
 1789  vi A4_script.sh 
 1790  ./A4_script.sh
 1791  sort review_body
 1792  review_body < sort 
 1793  sort < review_body
 1794  head tweet_text
 1795  head tweet_text2.txt 
 1796  sort tweet_text2.txt > tweet_text.txt 
 1797  head tweet_text2.txt
 1798  ./A4_script.sh
 1799  sort review_body > review_body2.txt
 1800  ./A4_script.sh
 1801  vi A4_script.sh 
 1802  vi A4_script.sh tweet_text.txt review_body2.txt
 1803  comm -12 tweet_text.txt review_body2.txt
 1804  head tweet_text.txt
 1805  head review_body2.txt 
 1806  head tweet_text2.txt
 1807  comm -12 tweet_text2.txt review_body2.txt
 1808  sort tweet_text2.txt | head
 1809  head tweet_text2.txt| sort | head
 1810  head 100 tweet_text2.txt| sort | head
 1811  head tweet_text2.txt| sort | head
 1812  head tweet_text2.txt| sort | head > tweet_text.txt
 1813  head tweet_text.txt
 1814  comm -12 tweet_text.txt review_body2.txt
 1815  head tweet_text.txt
 1816  head review_body2.txt
 1817  vi tweet_text.txt
 1818  comm -12 tweet_text.txt review_body2.txt
 1819  vi tweet_text.txt
 1820  comm -12 tweet_text.txt review_body2.txt
 1821  time comm -12 tweet_text.txt review_body2.txt
 1822  sed -i 's/\s\+/\n/g' review_body2.txt 
 1823  time comm -12 tweet_text.txt review_body2.txt
 1824  comm -12 tweet_text.txt review_body2.txt
 1825  sort review_body2.txt > review_body3.txt
 1826  comm -12 tweet_text.txt review_body3.txt
 1827  script a4.txt
 1828  .txt
 1829  phans@f6linux17:~$
 1830  .txt
 1831  phans@f6linux17:~$
 1832  git clone https://github.com/S-phan/Assassignment-4.git
 1833  cp a4.txt Assassignment-4
 1834  cd Assassignment-4
 1835  git status
 1836  git add .
 1837  git commit -m "Assignment 4"
 1838  git push https://github.com/S-phan/Assassignment-4.git
 1839  mkdir REVIEWS
 1840  head -n 101 amazon_reviews_us_Books_v1_02.tsv | grep -v helpful_vo > 100_amazon.txt
 1841  head 100_amazon.txt 
 1842  sort -n -k 9 100_amazon.txt 
 1843  sort -t "	" -n -k 9 100_amazon.txt | tail - 10 | awk -F "\t" {printf "%s,%s\n", $13, $14 > "REVIEW/" $2 ".txt"}
 1844  sort -t "	" -n -k 9 100_amazon.txt | tail - 10 | awk -F "\t" {printf "%s,%s\n", $13, $14 > "REVIEWS/" $2 ".txt"}
 1845  sort -t "	" -n -k 9 100_amazon.txt | tail - 10 | awk -F "\t" '{printf "%s,%s\n", $13, $14 > "REVIEWS/" $2 ".txt"}'
 1846  sort -t "	" -n -k 9 100_amazon.txt | tail -10 | awk -F "\t" '{printf "%s,%s\n", $13, $14 > "REVIEWS/" $2 ".txt"}'
 1847  ls -latr REVIEWS
 1848  cd REVIEWS
 1849  ls
 1850  cd ..
 1851  for i in {1..100} ; do echo $i; sed -n "${i}p" training.1600000.processed.noemoticon.csv > tweet.$i.csv ; done
 1852   
 1853  head tweet_set.txt
 1854  for i in {1..100} ; do echo $i; sed -n "${i}p" tweet_set.txt > tweet.$i.csv ; done
 1855  for i in {1..100} ; do echo $i; sed -n "${i}p" tweet_set.txt | awk -F "," '{print $6}'> tweet.$i.csv ; done
 1856  vi tweet.100.csv
 1857  for i in {1..100} ; do sed -n "${i}p"; tweet_set| awk -F "\",\"" '{print $6}' | sed 's/^"//g' | sed 's/"$//g' > tweet.$i.csv ; done
 1858  for i in {1..100} ; do sed -n "${i}p" tweet_set.txt| awk -F "\",\"" '{print $6}' | sed 's/^"//g' | sed 's/"$//g' > tweet.$i.csv ; done
 1859  vi tweet.100.csv
 1860  head tweet_set.txt
 1861  for i in {1..100} ; do sed -n "${i}p" tweet_set.txt| awk -F "\",\"" '{print $6}' | sed 's/^"//g' | sed 's/"$//g' > tweet.$i.csv ; done
 1862  ls
 1863  vi A4_script.sh 
 1864  ./A4_script.sh 
 1865  vi A4_script.sh 
 1866  ./A4_script.sh 
 1867  ./A4_script.sh 
 1868  vi A4_script.sh 
 1869  bunzip2 parallel-latest.tar.bz2
 1870  sudo apt install parallel
 1871  echo
 1872  script a4.txt
 1873  cp a4.txt Assassignment-4/
 1874  history > cmds.log 
 1875  cp cmds.log Assassignment-4/ 
 1876  cd Assassignment-4/
 1877  ls
 1878  git status
 1879  git add .
 1880  git commit " revised hw"
 1881  git commit -m " revised hw"
 1882  git push https://github.com/S-phan/Assassignment-4.git
 1883  vi randomsample.sh 
 1884  vi randomsample.sh 1 2
 1885  ./randomsample.sh 1 2
 1886  vi randomsample.sh 1 2
 1887  ./randomsample.sh 1 2
 1888  vi randomsample.sh 1 2
 1889  ./randomsample.sh 2 2
 1890  ./randomsample.sh 1 2
 1891  vi randomsample.sh 1 2
 1892  ./randomsample.sh 1 2
 1893  vi randomsample.sh 1 2
 1894  ./randomsample.sh 1 2
 1895  vi randomsample.sh 1 2
 1896  ./randomsample.sh 1 2
 1897  vi randomsample.s
 1898  ./randomsample.sh 1 2
 1899  ./randomsample.sh 3 3
 1900  vi randomsample.sh 1 2
 1901  ./randomsample.sh
 1902  vi randomsample.sh
 1903  ./randomsample.sh 1 2
 1904  vi randomsample.sh
 1905  ./randomsample.sh 1 2
 1906  vi randomsample.sh
 1907  ./randomsample.sh 1 2
 1908  vi randomsample.sh
 1909  ./randomsample.sh 3 3
 1910  vi randomsample.sh
 1911  ./randomsample.sh 3 3
 1912  vi randomsample.sh
 1913  ./randomsample.sh 3 3
 1914  vi randomsample.sh
 1915  ./randomsample.sh 3 3
 1916  vi randomsample.sh
 1917  ./randomsample.sh 3 3
 1918  vi randomsample.sh
 1919  ./randomsample.sh 3 3
 1920  vi randomsample.sh
 1921  ./randomsample.sh 3 3
 1922  vi randomsample.sh
 1923  ./randomsample.sh 3 3
 1924  wc -l amazon_reviews_us_Books_v1_02.tsv 
 1925  vi randomsample.sh
 1926  ./randomsample.sh 3 3 amazon_reviews_us_Books_v1_02.tsv 
 1927  vi randomsample.sh
 1928  ./randomsample.sh  amazon_reviews_us_Books_v1_02.tsv 
 1929  vi randomsample.sh
 1930  ./randomsample.sh  amazon_reviews_us_Books_v1_02.tsv 2 
 1931  vi randomsample.sh
 1932  vi randomsample.sh
 1933  chmod u+x randomsample.sh 
 1934  ./randomsample.sh
 1935  ./randomsample.sh 1 2
 1936  vi randomsample.sh
 1937  ./randomsample.sh
 1938  ./randomsample.sh 1 2
 1939  vi randomsample.sh
 1940  vi randomsample.sh 1 2
 1941  ./randomsample.sh 1 2
 1942  vi randomsample.sh
 1943  ./randomsample.sh 1 2
 1944  vi randomsample.sh
 1945  ./randomsample.sh
 1946  vi randomsample.sh
 1947  ./randomsample.sh
 1948  ./randomsample.sh 3 3 
 1949  vi randomsample.sh
 1950  wc -l amazon_reviews_us_Books_v1_02.tsv | 'print "$1"
 1951  wc -l amazon_reviews_us_Books_v1_02.tsv |awk 'print "$1"
 1952  wc -l amazon_reviews_us_Books_v1_02.tsv |awk {'print "$1"}
 1953  wc -l amazon_reviews_us_Books_v1_02.tsv |awk {'print "$1"'}
 1954  wc -l amazon_reviews_us_Books_v1_02.tsv |cut -f1 -d' '`
 1955  ls
 1956  wc -l test3.txt |cut -f1 -d' '`
 1957  wc -l review_body.txt |cut -f1 -d' '`
 1958  vi randomsample.sh
 1959  ./randomsample.sh review_body.txt 3 
 1960  vi randomsample.sh
 1961  ./randomsample.sh review_body.txt 3 
 1962  vi randomsample.sh
 1963  ./randomsample.sh review_body.txt 3 
 1964  wc -l review_body.txt |cut -f1 -d' '`
 1965  ./randomsample.sh review_body.txt 3
 1966  wc -l review_body.txt |cut -f1 -d' '
 1967  ./randomsample.sh review_body.txt 
 1968  ./randomsample.sh 
 1969  vi randomsample.sh 
 1970  ./randomsample.sh review_body.txt 3
 1971  vi randomsample.sh 
 1972  ./randomsample.sh review_body.txt 
 1973  vi randomsample.sh 
 1974  ./randomsample.sh review_body.txt 
 1975  vi randomsample.sh 
 1976  ./randomsample.sh review_body.txt 
 1977  vi randomsample.sh 
 1978  ./randomsample.sh review_body.txt 
 1979  vi randomsample.sh 
 1980  echo $(($RANDOM % 100))
 1981  vi randomsample.sh 
 1982  ./randomsample.sh 10 100_amazon.txt
 1983  ./randomsample.sh 49 100_amazon.txt
 1984  echo $(($RANDOM % 100))
 1985  ./randomsample.sh 22 100_amazon.txt
 1986  history > cmd.log
